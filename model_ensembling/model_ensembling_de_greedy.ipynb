{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added: coatnet | Val AUROC: 0.8278\n",
      "Added: convnext | Val AUROC: 0.8368\n",
      "Added: densenet | Val AUROC: 0.8410\n",
      "Added: maxvit | Val AUROC: 0.8429\n",
      "Added: vgg19 | Val AUROC: 0.8438\n",
      "Added: swin | Val AUROC: 0.8444\n",
      "\n",
      "Final Test Ensemble Evaluation:\n",
      "Selected Models: ['coatnet', 'convnext', 'densenet', 'maxvit', 'vgg19', 'swin']\n",
      "Optimized Weights:\n",
      "coatnet: 0.1524\n",
      "convnext: 0.1877\n",
      "densenet: 0.2052\n",
      "maxvit: 0.1663\n",
      "vgg19: 0.1138\n",
      "swin: 0.1747\n",
      "\n",
      "Mean Test AUROC: 0.8563\n",
      "\n",
      "Per-Class Test AUROC:\n",
      "No Finding: 0.8017\n",
      "Atelectasis: 0.8355\n",
      "Cardiomegaly: 0.9153\n",
      "Effusion: 0.8950\n",
      "Infiltration: 0.7376\n",
      "Mass: 0.8820\n",
      "Nodule: 0.8097\n",
      "Pneumonia: 0.7871\n",
      "Pneumothorax: 0.8963\n",
      "Consolidation: 0.8187\n",
      "Edema: 0.9156\n",
      "Emphysema: 0.9449\n",
      "Fibrosis: 0.8479\n",
      "Pleural_Thickening: 0.8424\n",
      "Hernia: 0.9146\n"
     ]
    }
   ],
   "source": [
    "# (Greedy forward selection + DE-based weight tuning on val + Test set AUROC eval only)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.optimize import differential_evolution\n",
    "import timm\n",
    "\n",
    "# ------------------------\n",
    "# 1. Define Disease Classes\n",
    "# ------------------------\n",
    "CLASSES = [\n",
    "    \"No Finding\", \"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\",\n",
    "    \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\", \"Consolidation\",\n",
    "    \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# 2. Load and Preprocess Data\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"/student/csc490_project/shared/labels.csv\")\n",
    "df[\"label_list\"] = df[\"Finding Labels\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=CLASSES)\n",
    "labels_array = mlb.fit_transform(df[\"label_list\"])\n",
    "df[\"labels\"] = list(labels_array)\n",
    "\n",
    "unique_patients = df[\"Patient ID\"].unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_patients)\n",
    "\n",
    "train_end = int(0.7 * len(unique_patients))\n",
    "val_end = int(0.8 * len(unique_patients))\n",
    "\n",
    "train_patients = unique_patients[:train_end]\n",
    "val_patients = unique_patients[train_end:val_end]\n",
    "test_patients = unique_patients[val_end:]\n",
    "\n",
    "val_df = df[df[\"Patient ID\"].isin(val_patients)].reset_index(drop=True)\n",
    "test_df = df[df[\"Patient ID\"].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# ------------------------\n",
    "# 3. Dataset Class\n",
    "# ------------------------\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading chest X-ray images and corresponding multilabel disease annotations.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): DataFrame with image filenames and labels.\n",
    "        root_dir (str): Directory containing image files.\n",
    "        transform (callable): Image transformations to apply.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df.iloc[idx][\"Image Index\"])\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        labels = torch.tensor(self.df.iloc[idx][\"labels\"], dtype=torch.float)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "# ------------------------\n",
    "# 4. Transforms and DataLoaders\n",
    "# ------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img_dir = \"/student/csc490_project/shared/preprocessed_images/preprocessed_images\"\n",
    "val_loader = DataLoader(ChestXrayDataset(val_df, img_dir, transform), batch_size=16, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(ChestXrayDataset(test_df, img_dir, transform), batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "# ------------------------\n",
    "# 5. Load Pretrained Models\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = {\n",
    "    'maxvit': timm.create_model('maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k', pretrained=False, num_classes=15),\n",
    "    'densenet': timm.create_model('densenet121', pretrained=False, num_classes=15),\n",
    "    'coatnet': timm.create_model('coatnet_2_rw_224.sw_in12k_ft_in1k', pretrained=False, num_classes=15),\n",
    "    'swin': timm.create_model('swin_large_patch4_window7_224', pretrained=False, num_classes=15),\n",
    "    'convnext': timm.create_model('convnext_large.fb_in22k', pretrained=False, num_classes=15),\n",
    "    'vgg19': timm.create_model('vgg19.tv_in1k', pretrained=False, num_classes=15)\n",
    "}\n",
    "\n",
    "models['maxvit'].load_state_dict(torch.load('/student/csc490_project/shared/new_split_models/no_augment_maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k_model.pth'))\n",
    "models['densenet'].load_state_dict(torch.load('/student/csc490_project/shared/new_split_models/no_augment_densenet121_model.pth'))\n",
    "models['coatnet'].load_state_dict(torch.load('/student/csc490_project/shared/new_split_models/no_augment_coatnet_2_rw_224.sw_in12k_ft_in1k_model.pth'))\n",
    "models['swin'].load_state_dict(torch.load('/student/csc490_project/shared/new_split_models/no_augment_swin_large_patch4_window7_224_model.pth'))\n",
    "models['convnext'].load_state_dict(torch.load('/student/csc490_project/shared/new_split_models/no_augment_convnext_large.fb_in22k_model.pth'))\n",
    "models['vgg19'].load_state_dict(torch.load('/student/csc490_project/shared/training/vgg19.tv_in1k.pt'))\n",
    "\n",
    "for model in models.values():\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# 6. Collect Predictions\n",
    "# ------------------------\n",
    "def collect_predictions(loader, models, device):\n",
    "    \"\"\"\n",
    "    Runs inference using multiple models on the input DataLoader.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): DataLoader containing input images and labels.\n",
    "        models (dict): Dictionary of model name to model instance.\n",
    "        device (torch.device): Device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionary of model predictions and array of ground-truth labels.\n",
    "    \"\"\"\n",
    "    all_preds = {name: [] for name in models}\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            for name, model in models.items():\n",
    "                all_preds[name].append(torch.sigmoid(model(images)).cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = {k: np.concatenate(v) for k, v in all_preds.items()}\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# ------------------------\n",
    "# 7. Weighted AUROC Objective\n",
    "# ------------------------\n",
    "def weighted_ensemble_auroc(weights, preds_list, labels):\n",
    "    \"\"\"\n",
    "    Objective function for differential evolution: negative average AUROC.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): Ensemble weights to apply to predictions.\n",
    "        preds_list (list): List of prediction arrays from different models.\n",
    "        labels (np.ndarray): Ground-truth labels.\n",
    "\n",
    "    Returns:\n",
    "        float: Negative of mean AUROC score across all classes.\n",
    "    \"\"\"\n",
    "    weights = np.maximum(weights, 0)\n",
    "    weights /= np.sum(weights) + 1e-8\n",
    "    ensemble_preds = sum(w * p for w, p in zip(weights, preds_list))\n",
    "    return -np.mean([\n",
    "        roc_auc_score(labels[:, i], ensemble_preds[:, i]) for i in range(labels.shape[1])\n",
    "    ])\n",
    "\n",
    "# ------------------------\n",
    "# 8. DE Weight Optimization\n",
    "# ------------------------\n",
    "def optimize_weights_subset(preds_dict, labels, model_subset):\n",
    "    \"\"\"\n",
    "    Optimizes ensemble weights for a subset of models using Differential Evolution.\n",
    "\n",
    "    Args:\n",
    "        preds_dict (dict): Dictionary of model name to prediction arrays.\n",
    "        labels (np.ndarray): Ground-truth labels.\n",
    "        model_subset (list): List of model names to include in ensemble.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Optimized weights, mean AUROC, final predictions, and per-class AUROC list.\n",
    "    \"\"\"\n",
    "    preds_list = [preds_dict[name] for name in model_subset]\n",
    "    bounds = [(0, 1)] * len(preds_list)\n",
    "    result = differential_evolution(weighted_ensemble_auroc, bounds, args=(preds_list, labels), maxiter=50, tol=1e-5)\n",
    "    best_weights = result.x / np.sum(result.x)\n",
    "    final_preds = sum(w * p for w, p in zip(best_weights, preds_list))\n",
    "    per_class_aurocs = [roc_auc_score(labels[:, i], final_preds[:, i]) for i in range(labels.shape[1])]\n",
    "    final_score = np.mean(per_class_aurocs)\n",
    "    return best_weights, final_score, final_preds, per_class_aurocs\n",
    "\n",
    "# ------------------------\n",
    "# 9. Greedy Forward Model Selection\n",
    "# ------------------------\n",
    "def greedy_forward_selection(preds_dict, labels):\n",
    "    \"\"\"\n",
    "    Performs greedy forward selection of models based on validation AUROC.\n",
    "\n",
    "    Args:\n",
    "        preds_dict (dict): Dictionary of model predictions.\n",
    "        labels (np.ndarray): Ground-truth labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: List of selected model names and best AUROC score.\n",
    "    \"\"\"\n",
    "    model_names = list(preds_dict.keys())\n",
    "    remaining_models = set(model_names)\n",
    "    selected_models = []\n",
    "    best_score = -np.inf\n",
    "\n",
    "    while remaining_models:\n",
    "        best_model = None\n",
    "        best_score_candidate = -np.inf\n",
    "\n",
    "        for model in remaining_models:\n",
    "            current_combo = selected_models + [model]\n",
    "            weights, score, _, _ = optimize_weights_subset(preds_dict, labels, current_combo)\n",
    "            if score > best_score_candidate:\n",
    "                best_score_candidate = score\n",
    "                best_model = model\n",
    "\n",
    "        if best_score_candidate > best_score:\n",
    "            selected_models.append(best_model)\n",
    "            remaining_models.remove(best_model)\n",
    "            best_score = best_score_candidate\n",
    "            print(f\"Added: {best_model} | Val AUROC: {best_score:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return selected_models, best_score\n",
    "\n",
    "# ------------------------\n",
    "# 10. Ensemble Optimization and Final Evaluation\n",
    "# ------------------------\n",
    "val_preds, val_labels = collect_predictions(val_loader, models, device)\n",
    "selected_models, _ = greedy_forward_selection(val_preds, val_labels)\n",
    "final_weights, _, _, _ = optimize_weights_subset(val_preds, val_labels, selected_models)\n",
    "\n",
    "test_preds, test_labels = collect_predictions(test_loader, models, device)\n",
    "ensemble_test_preds = sum(w * test_preds[name] for w, name in zip(final_weights, selected_models))\n",
    "\n",
    "per_class_aurocs = [roc_auc_score(test_labels[:, i], ensemble_test_preds[:, i]) for i in range(test_labels.shape[1])]\n",
    "mean_test_auroc = np.mean(per_class_aurocs)\n",
    "\n",
    "# ------------------------\n",
    "# 11. Display Results\n",
    "# ------------------------\n",
    "print(\"\\nFinal Test Ensemble Evaluation:\")\n",
    "print(f\"Selected Models: {selected_models}\")\n",
    "print(\"Optimized Weights:\")\n",
    "for name, weight in zip(selected_models, final_weights):\n",
    "    print(f\"{name}: {weight:.4f}\")\n",
    "\n",
    "print(f\"\\nMean Test AUROC: {mean_test_auroc:.4f}\")\n",
    "print(\"\\nPer-Class Test AUROC:\")\n",
    "for cls, auc in zip(CLASSES, per_class_aurocs):\n",
    "    print(f\"{cls}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Ensemble Evaluation with TTA:\n",
      "Selected Models: ['coatnet', 'convnext', 'densenet', 'maxvit', 'vgg19', 'swin']\n",
      "Optimized Weights:\n",
      "coatnet: 0.1524\n",
      "convnext: 0.1877\n",
      "densenet: 0.2052\n",
      "maxvit: 0.1663\n",
      "vgg19: 0.1138\n",
      "swin: 0.1747\n",
      "\n",
      "Mean Test AUROC (TTA): 0.8577\n",
      "\n",
      "Per-Class Test AUROC (TTA):\n",
      "No Finding: 0.8021\n",
      "Atelectasis: 0.8372\n",
      "Cardiomegaly: 0.9170\n",
      "Effusion: 0.8953\n",
      "Infiltration: 0.7376\n",
      "Mass: 0.8829\n",
      "Nodule: 0.8108\n",
      "Pneumonia: 0.7873\n",
      "Pneumothorax: 0.8971\n",
      "Consolidation: 0.8193\n",
      "Edema: 0.9158\n",
      "Emphysema: 0.9454\n",
      "Fibrosis: 0.8473\n",
      "Pleural_Thickening: 0.8443\n",
      "Hernia: 0.9257\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 12. Test-Time Augmentation Evaluation\n",
    "# ------------------------\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define TTA transforms: each transform applies a different augmentation strategy during test time\n",
    "tta_transforms = [\n",
    "    transforms.Compose([  # Horizontal flip\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transforms.Compose([  # Brightness and contrast jitter\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transforms.Compose([  # Slight translation\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.02, 0.02)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transform  # Original (no augmentation)\n",
    "]\n",
    "\n",
    "def collect_tta_predictions(df, root_dir, models, device, tta_transforms, batch_size=16):\n",
    "    \"\"\"\n",
    "    Collects model predictions under various test-time augmentations and averages them.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing test image metadata.\n",
    "        root_dir (str): Directory where images are stored.\n",
    "        models (dict): Dictionary mapping model names to PyTorch model instances.\n",
    "        device (torch.device): Device to perform inference on (e.g., \"cuda\" or \"cpu\").\n",
    "        tta_transforms (list): List of torchvision.transforms.Compose instances for TTA.\n",
    "        batch_size (int, optional): Batch size for DataLoader. Defaults to 16.\n",
    "\n",
    "    Returns:\n",
    "        dict: Averaged predictions per model across all TTAs.\n",
    "        np.ndarray: Ground-truth labels (assumed constant across TTAs).\n",
    "    \"\"\"\n",
    "    # Initialize storage for predictions and labels\n",
    "    all_preds = {name: [] for name in models}\n",
    "    all_labels_tta = []\n",
    "\n",
    "    # Run predictions across all TTA transforms\n",
    "    for tform in tta_transforms:\n",
    "        loader = DataLoader(\n",
    "            ChestXrayDataset(df, root_dir, tform),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "        preds_this_tta = {name: [] for name in models}\n",
    "        labels_this_tta = []\n",
    "\n",
    "        # Collect predictions for this TTA setting\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                for name, model in models.items():\n",
    "                    outputs = model(images)\n",
    "                    preds = torch.sigmoid(outputs)  # Apply sigmoid for multi-label probabilities\n",
    "                    preds_this_tta[name].append(preds.cpu().numpy())\n",
    "\n",
    "                labels_this_tta.append(labels.cpu().numpy())\n",
    "\n",
    "        # Concatenate and store predictions\n",
    "        for name in models:\n",
    "            concatenated_preds = np.concatenate(preds_this_tta[name], axis=0)\n",
    "            all_preds[name].append(concatenated_preds)\n",
    "\n",
    "        # Store labels (assumed identical for all TTAs)\n",
    "        concatenated_labels = np.concatenate(labels_this_tta, axis=0)\n",
    "        all_labels_tta.append(concatenated_labels)\n",
    "\n",
    "    # Sanity check: ensure label consistency across all TTA transforms\n",
    "    for i in range(1, len(all_labels_tta)):\n",
    "        if all_labels_tta[i].shape != all_labels_tta[0].shape or not np.array_equal(all_labels_tta[i], all_labels_tta[0]):\n",
    "            raise ValueError(\"Label mismatch across TTAs — check consistency of test DataLoader.\")\n",
    "\n",
    "    all_labels = all_labels_tta[0]  # Use first as ground truth\n",
    "\n",
    "    # Average predictions across TTAs for each model\n",
    "    all_preds_avg = {}\n",
    "    for name in models:\n",
    "        stacked_preds = np.stack(all_preds[name], axis=0)  # Shape: (num_tta, num_samples, num_classes)\n",
    "        all_preds_avg[name] = np.mean(stacked_preds, axis=0)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "    return all_preds_avg, all_labels\n",
    "\n",
    "# Run Test-Time Augmentation on test set\n",
    "test_preds_tta, test_labels_tta = collect_tta_predictions(test_df, img_dir, models, device, tta_transforms)\n",
    "\n",
    "# Perform ensemble prediction using DE-tuned weights\n",
    "ensemble_test_preds_tta = sum(w * test_preds_tta[name] for w, name in zip(final_weights, selected_models))\n",
    "\n",
    "# Compute per-class AUROC scores with TTA\n",
    "per_class_aurocs_tta = [\n",
    "    roc_auc_score(test_labels_tta[:, i], ensemble_test_preds_tta[:, i])\n",
    "    for i in range(test_labels_tta.shape[1])\n",
    "]\n",
    "mean_test_auroc_tta = np.mean(per_class_aurocs_tta)\n",
    "\n",
    "# Print final TTA results\n",
    "print(\"\\nFinal Test Ensemble Evaluation with TTA:\")\n",
    "print(f\"Selected Models: {selected_models}\")\n",
    "print(\"Optimized Weights:\")\n",
    "for name, weight in zip(selected_models, final_weights):\n",
    "    print(f\"{name}: {weight:.4f}\")\n",
    "print(f\"\\nMean Test AUROC (TTA): {mean_test_auroc_tta:.4f}\")\n",
    "print(\"\\nPer-Class Test AUROC (TTA):\")\n",
    "for cls, auc in zip(CLASSES, per_class_aurocs_tta):\n",
    "    print(f\"{cls}: {auc:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
