{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "import torch\n",
    "import time\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 112120 images.\n",
      "Initial dataset shape: (112120, 17)\n",
      "Rare classes with <2 samples: []\n",
      "Updated dataset shape after removing rare classes: (112120, 17)\n",
      "Training set contains 78566 images.\n",
      "Test set contains 33554 images.\n",
      "Training set contains 78566 images from 21563 unique patients.\n",
      "Test set contains 33554 images from 9242 unique patients.\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/student/csc490_project/shared/labels.csv\"\n",
    "image_dir = \"/student/csc490_project/shared/preprocessed_images/preprocessed_images\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Keep only rows where 'Image Index' is in preprocessed images\n",
    "preprocessed_images = set(os.listdir(image_dir))\n",
    "df = df[df[\"Image Index\"].isin(preprocessed_images)]\n",
    "print(f\"Filtered dataset contains {len(df)} images.\")\n",
    "\n",
    "# Convert 'Finding Labels' into a list of diseases\n",
    "df[\"Finding Labels\"] = df[\"Finding Labels\"].apply(lambda x: x.split('|'))\n",
    "\n",
    "# Convert labels into a binary multi-label format\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = pd.DataFrame(mlb.fit_transform(df[\"Finding Labels\"]), columns=mlb.classes_)\n",
    "\n",
    "# Merge binary labels with the dataset and keep 'Image Index', 'Patient ID', and label columns\n",
    "df = df.join(labels)\n",
    "df = df[['Image Index', 'Patient ID'] + list(mlb.classes_)]\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "\n",
    "# Remove rare diseases with fewer than 2 samples (note: labels start at column index 2 now)\n",
    "class_counts = df.iloc[:, 2:].sum()\n",
    "rare_classes = class_counts[class_counts < 2].index.tolist()\n",
    "print(f\"Rare classes with <2 samples: {rare_classes}\")\n",
    "df = df.drop(columns=rare_classes)\n",
    "\n",
    "# Remove rows where all labels are 0 (images that only had rare labels)\n",
    "df = df[df.iloc[:, 2:].sum(axis=1) > 0]\n",
    "print(f\"Updated dataset shape after removing rare classes: {df.shape}\")\n",
    "\n",
    "# Split the dataset into training and test sets ensuring no patient appears in both sets\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(df, groups=df[\"Patient ID\"]))\n",
    "print(f\"Training set contains {len(train_idx)} images.\")\n",
    "print(f\"Test set contains {len(test_idx)} images.\")\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "# Verify that there is no overlap in patient IDs between training and test sets\n",
    "print(f\"Training set contains {len(train_df)} images from {train_df['Patient ID'].nunique()} unique patients.\")\n",
    "print(f\"Test set contains {len(test_df)} images from {test_df['Patient ID'].nunique()} unique patients.\")\n",
    "\n",
    "common_patients = set(train_df[\"Patient ID\"]).intersection(set(test_df[\"Patient ID\"]))\n",
    "assert len(common_patients) == 0, \"OVERLAP OF PATIENTS BETWEEN TRAINING AND TEST SETS\"\n",
    "\n",
    "# Define disease labels (all columns except 'Image Index' and 'Patient ID')\n",
    "disease_labels = list(train_df.columns[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(row): \n",
    "    return os.path.join(image_dir, row['Image Index'])\n",
    "\n",
    "def get_y(row):\n",
    "    return [label for label, value in zip(disease_labels, row[disease_labels]) if value == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transform for Gamma Correction\n",
    "class GammaCorrection(Transform):\n",
    "    def __init__(self, gamma:float=1.0):\n",
    "        self.gamma = gamma\n",
    "    def encodes(self, img:PILImage):\n",
    "        # Expecting a grayscale PIL image.\n",
    "        img_np = np.array(img).astype(np.float32) / 255.0\n",
    "        corrected = np.power(img_np, self.gamma)\n",
    "        corrected = np.clip(corrected * 255, 0, 255).astype(np.uint8)\n",
    "        return PILImage.create(corrected)\n",
    "\n",
    "# Transform to convert image to 3 channels\n",
    "def to_3channel(img:PILImage):\n",
    "    return img.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataBlock for multi-label classification.\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, MultiCategoryBlock(vocab=disease_labels)),\n",
    "    get_x=get_x,\n",
    "    get_y=get_y,\n",
    "    splitter=IndexSplitter(test_idx),\n",
    "    item_tfms=[GammaCorrection(gamma=0.8), to_3channel, Resize(224)],\n",
    "    batch_tfms=[*aug_transforms(flip_vert=False, max_rotate=15, max_zoom=1.0, max_warp=0.),\n",
    "                Normalize.from_stats([0.485,0.456,0.406],[0.229,0.224,0.225])]\n",
    ")\n",
    "\n",
    "# Create DataLoaders from the full DataFrame\n",
    "dls = dblock.dataloaders(df, bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'convnext_large.fb_in22k'\n",
    "num_classes = len(disease_labels)  # This should match the number of labels\n",
    "model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_func = BCEWithLogitsLossFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>acc_multi</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.182011</td>\n",
       "      <td>0.184758</td>\n",
       "      <td>0.933025</td>\n",
       "      <td>0.080393</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>21:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169963</td>\n",
       "      <td>0.185421</td>\n",
       "      <td>0.932763</td>\n",
       "      <td>0.120594</td>\n",
       "      <td>0.815698</td>\n",
       "      <td>21:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.168919</td>\n",
       "      <td>0.175138</td>\n",
       "      <td>0.934535</td>\n",
       "      <td>0.163524</td>\n",
       "      <td>0.831182</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172457</td>\n",
       "      <td>0.173398</td>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>0.836062</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.163760</td>\n",
       "      <td>0.172804</td>\n",
       "      <td>0.934599</td>\n",
       "      <td>0.209899</td>\n",
       "      <td>0.839309</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.156674</td>\n",
       "      <td>0.176435</td>\n",
       "      <td>0.933562</td>\n",
       "      <td>0.197463</td>\n",
       "      <td>0.835497</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.150029</td>\n",
       "      <td>0.182624</td>\n",
       "      <td>0.930961</td>\n",
       "      <td>0.255974</td>\n",
       "      <td>0.827089</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.124782</td>\n",
       "      <td>0.198783</td>\n",
       "      <td>0.926501</td>\n",
       "      <td>0.228735</td>\n",
       "      <td>0.800065</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with roc_auc_score value: 0.7943999687559972.\n",
      "Better model found at epoch 1 with roc_auc_score value: 0.8156983275798634.\n",
      "Better model found at epoch 2 with roc_auc_score value: 0.8311822134934066.\n",
      "Better model found at epoch 3 with roc_auc_score value: 0.8360619922950382.\n",
      "Better model found at epoch 4 with roc_auc_score value: 0.8393086681500379.\n",
      "No improvement since epoch 4: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/sidd1091/.local/lib/python3.11/site-packages/fastai/learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(file, map_location=device, **torch_load_kwargs)\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=loss_func,\n",
    "    metrics=[\n",
    "        AccumMetric(accuracy_multi, name='acc_multi'),  \n",
    "        F1ScoreMulti(),                                 \n",
    "        RocAucMulti()                                   \n",
    "    ]\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(\n",
    "    20,\n",
    "    lr_max=1e-4,\n",
    "    cbs=[\n",
    "        SaveModelCallback(monitor='roc_auc_score', fname='noverlap_convnext'),\n",
    "        EarlyStoppingCallback(monitor='roc_auc_score', patience=3)\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
